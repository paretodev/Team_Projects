{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Engineering Math : Final Project (Han seokhee, Chung hyelee, H. Jongho)\n",
    "<h2><font color='red'>Title : < Using Socio-economic Signals to predict the income ></font></h2>    \n",
    "    \n",
    "### [Project_overview : Come here and watch overview pdf brochure before going down for code](https://drive.google.com/file/d/1xWQrm-9VBgGtEUKNsDHqPv4m1axLx0o1/view?usp=sharing)\n",
    "#### 1. We used UCI ‘Adult Data set’ (http://archive.ics.uci.edu/ml/datasets/Adult) which shows 14 socio-economic indicators such as marital status, profession and corresponding income in binary form(‘>50k’,’<=50k’), having about 50,000 instances.\n",
    "#### 2. (1).We tried to do Linear Regression from 14 socio-economic indicators to the binary values of ‘>50k’, ‘<=50k’, to know which factors are given which weights. By looking into the weights the Linear Regression model produces, we could know which signal is considered important in predicting income. (2). Then, we tried logistic regression to make a prediction model to tell us whether a person of certain values in 14 indicators is highly likely to earn that income or not. If the data is well-clustered in a binary sense, this simple model will function properly. (3). Finally, in order to make more subtle classification model, we tried Deep Neural Network classification to boost the accuracy in classification.\n",
    "#### 3. We thought this is interesting because : (1). If we apply this examination into many other years’ census data, we could track the change of the importance given to each indicators signaling income in timeline. (2). The similar linear regression and prediction model approach could be applied practically in welfare policy making and target marketing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><font color='red'>1. Data Preprocessing </font></h2>\n",
    "\n",
    "1. First, we divided non-sequential and string- categorical columns to separate columns that each represent the whole categories that belong to that original column. We also singled out ‘null’ column to prevent data loss(14 columns and omitting the null- including instance was fatal). For instance, occupation column was divided into 14 independent one-hot columns.\n",
    "2. Second, integer valued columns(age, education level, weekly working hours) were z-score normalized for effective gradients update for every column. However, there were the integer valued columns that did not have normal distribution but rather have the distribution similar to that of power law. Capital-gain and capital-loss column were such columns. For those columns we first scaled them down by log of base 10 and then applied z-score, to counter the different distribution. Fortunately, these columns didn’t have undefined values.\n",
    "3. To sum up, as a results of dividing string category columns, our dataset matrix came to have 91 columns with the same 48842 instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created file '/Users/sucky/Desktop/Team_Projects/19-1R-ENGINEERING_MATH_TeamProjects/Final_Term_Project/cat2numeric.m'.\n"
     ]
    }
   ],
   "source": [
    "%%file cat2numeric.m\n",
    "function x = cat2numeric(A)\n",
    "\n",
    "if size(A,2) ~= 15\n",
    "    error(\"check data size\")\n",
    "end\n",
    "%pre-processing the data\n",
    "x =zeros(size(A, 1), 91);\n",
    "%puts age\n",
    "x(:, 1) = zscore(A.age);\n",
    "%puts workClass\n",
    "temp = grp2idx(A.workClass);\n",
    "temp(isnan(temp)) = 1+size(categories(A.workClass),1);\n",
    "for i = 1 : size(A,1)\n",
    "    x(i, 1+temp(i)) = 1;\n",
    "end\n",
    "%puts education_num\n",
    "x(:, 11) = zscore(A.education_num);\n",
    "%puts marital_status\n",
    "temp = grp2idx(A.marital_status);\n",
    "temp(isnan(temp)) = 1+size(categories(A.marital_status),1);\n",
    "for i = 1 : size(A,1)\n",
    "    x(i, 11+temp(i)) = 1;\n",
    "end\n",
    "%puts occupation\n",
    "temp = grp2idx(A.occupation);\n",
    "temp(isnan(temp)) = 1+size(categories(A.occupation),1);\n",
    "for i = 1 : size(A,1)\n",
    "    x(i, 18+temp(i)) = 1;\n",
    "end\n",
    "%  puts relationship\n",
    "temp = grp2idx(A.relationship);\n",
    "temp(isnan(temp)) = 1+size(categories(A.relationship),1);\n",
    "for i = 1 : size(A,1)\n",
    "    x(i, 33+temp(i)) = 1;\n",
    "end\n",
    "% puts race\n",
    "temp = grp2idx(A.race);\n",
    "temp(isnan(temp)) = 1+size(categories(A.race),1);\n",
    "for i = 1 : size(A,1)\n",
    "    x(i, 39+temp(i)) = 1;\n",
    "end\n",
    "% puts sex\n",
    "x(:, 45) = grp2idx(A.sex)-1;\n",
    "% puts capital_gain\n",
    "temp=A.capital_gain;\n",
    "temp(temp~=0) = log10(temp(temp~=0));\n",
    "x(:, 46) = zscore(temp);\n",
    "% puts capital_loss\n",
    "temp=A.capital_loss;\n",
    "temp(temp~=0) = log10(temp(temp~=0));\n",
    "x(:, 47) = zscore(temp);\n",
    "% puts hours_per_week\n",
    "x(:, 48) = zscore(A.hours_per_week);\n",
    "% puts native_country\n",
    "temp = grp2idx(A.native_country);\n",
    "temp(isnan(temp)) = 1+size(categories(A.native_country),1);\n",
    "for i = 1 : size(A,1)\n",
    "    x(i, 48+temp(i)) = 1;\n",
    "end\n",
    "%puts salary ( 1 if higher than 50000, 0 if lower)\n",
    "x(:, 91) = (A.salary=='>50K');\n",
    "\n",
    "end\n",
    "\n",
    "%column# | category  | ignore the numbers | attributes\n",
    "%---------------------------------------------------\n",
    "%1   age  (17~90)                           AGE       Z-SCORED !!\n",
    "\n",
    "%2  federal-gov,                           WORKCLASS\n",
    "%3  local-gov\n",
    "%4  never-worked,\n",
    "%5  private\n",
    "%6  self-emp-inc\n",
    "%7  self-emp-not-inc\n",
    "%8  state-gov\n",
    "%9  without-pay\n",
    "%10 NumMissing (undefined)\n",
    "\n",
    "%11 education_num (1 ~16 )              EDUCATION NUM  Z-SCORED !!\n",
    "\n",
    "%12 Divorced                             MARITAL STATUS\n",
    "%13         Married-AF-spouse                \n",
    "%14            Married-civ-spouse         \n",
    "%15            Married-spouse-absent        \n",
    "%16            Never-married             \n",
    "%17            Separated                \n",
    "%18            Widowed\n",
    "%19         Adm-clerical                        OCCUPATION\n",
    "%             Armed-Forces                9    \n",
    "%             Craft-repair             4099    \n",
    "%             Exec-managerial          4066    \n",
    "%             Farming-fishing           994    \n",
    "%             Handlers-cleaners        1370    \n",
    "%             Machine-op-inspct        2002    \n",
    "%             Other-service            3295    \n",
    "%             Priv-house-serv           149    \n",
    "%             Prof-specialty           4140    \n",
    "%             Protective-serv           649    \n",
    "%             Sales                    3650    \n",
    "%             Tech-support              928    \n",
    "%             Transport-moving         1597    \n",
    "%33           NumMissing               1843  \n",
    "\n",
    "%34           Husband               13193            RELATIONSHIP\n",
    "%             Not-in-family          8305     \n",
    "%             Other-relative          981     \n",
    "%             Own-child              5068     \n",
    "%             Unmarried              3446     \n",
    "% 39          Wife                   1568 \n",
    "\n",
    "% 40          Amer-Indian-Eskimo            311       RACE\n",
    "%             Asian-Pac-Islander     1039 \n",
    "%             Black                  3124 \n",
    "%             Other                   271 \n",
    "%             White                 27816 \n",
    "\n",
    "% 45          Sex(0 IF female, 1 if male)           SEX\n",
    "\n",
    "% 46          capital_gain (0~99999)            log10 the non-zeros then Z-SCORED !!\n",
    "\n",
    "% 47          capital_loss  (0~4356)            log10 the non-zeros then Z-SCORED !!\n",
    "\n",
    "% 48          hours per week (1~99)             Z-SCORED !!\n",
    "\n",
    "% 49          Cambodia                   19         NATIVE COUNTRY\n",
    "%50           Canada                               121      \n",
    "%             China                                 75      \n",
    "%             Columbia                              59      \n",
    "%             Cuba                                  95      \n",
    "%             Dominican-Republic                    70      \n",
    "%             Ecuador                               28      \n",
    "%             El-Salvador                          106      \n",
    "%             England                               90      \n",
    "%             France                                29      \n",
    "%             Germany                              137      \n",
    "%             Greece                                29      \n",
    "%             Guatemala                             64      \n",
    "%             Haiti                                 44      \n",
    "%             Holand-Netherlands                     1      \n",
    "%             Honduras                              13      \n",
    "%             Hong                                  20      \n",
    "%             Hungary                               13      \n",
    "%             India                                100      \n",
    "%             Iran                                  43      \n",
    "%             Ireland                               24      \n",
    "%             Italy                                 73      \n",
    "%             Jamaica                               81      \n",
    "%             Japan                                 62      \n",
    "%             Laos                                  18      \n",
    "%             Mexico                               643      \n",
    "%             Nicaragua                             34      \n",
    "%             Outlying-US(Guam-USVI-etc)            14      \n",
    "%             Peru                                  31      \n",
    "%             Philippines                          198      \n",
    "%             Poland                                60      \n",
    "%             Portugal                              37      \n",
    "%             Puerto-Rico                          114      \n",
    "%             Scotland                              12      \n",
    "%             South                                 80      \n",
    "%             Taiwan                                51      \n",
    "%             Thailand                              18      \n",
    "%             Trinadad&Tobago                       19      \n",
    "%             United-States                      29170      \n",
    "%             Vietnam                               67      \n",
    "%             Yugoslavia                            16      \n",
    "%  90         NumMissing  (undefined)              583      \n",
    "\n",
    "%  91         SALARY(0 if <=50K, 1 if >50k)                    SALARY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><font color='red'>2. Linear Regression </font></h2>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Our Linear Regression Algorithm accepts 91-dimension array(including one dimension for bias) as an input and tried to find a set of weights that minimizes the ‘the average of squared residual’ between batch datapoints and the labels. We used Gradient Descent to optimize the weights. (Compared this with the weight solution of Matlab ‘ \\ ’ to get the sense of goal accuracy of linear regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "iter =\n",
      "\n",
      "       37765\n",
      "\n",
      "\n",
      "l_2_norm_gradient =\n",
      "\n",
      "    0.0257\n",
      "\n",
      "\n",
      "iter =\n",
      "\n",
      "       75530\n",
      "\n",
      "\n",
      "l_2_norm_gradient =\n",
      "\n",
      "     0\n",
      "\n",
      "\n",
      "iter =\n",
      "\n",
      "      113295\n",
      "\n",
      "\n",
      "l_2_norm_gradient =\n",
      "\n",
      "    0.0043\n",
      "\n",
      "\n",
      "iter =\n",
      "\n",
      "      151060\n",
      "\n",
      "\n",
      "l_2_norm_gradient =\n",
      "\n",
      "     0\n",
      "\n",
      "\n",
      "iter =\n",
      "\n",
      "      188825\n",
      "\n",
      "\n",
      "l_2_norm_gradient =\n",
      "\n",
      "    0.0092\n",
      "\n",
      "\n",
      "iter =\n",
      "\n",
      "      226590\n",
      "\n",
      "\n",
      "l_2_norm_gradient =\n",
      "\n",
      "    0.0040\n",
      "\n",
      "\n",
      "iter =\n",
      "\n",
      "      264355\n",
      "\n",
      "\n",
      "l_2_norm_gradient =\n",
      "\n",
      "    0.0013\n",
      "\n",
      "Warning: Rank deficient, rank = 84, tol =  1.880902e-09.\n",
      "\n",
      "\n",
      "Linear_regression_accuracy =\n",
      "\n",
      "    84\n",
      "\n",
      "\n",
      "m_regression_accuracy =\n",
      "\n",
      "    84\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%% Load data : testing dataset & \n",
    "load census1994.mat\n",
    "\n",
    "numericdata = cat2numeric(adultdata);\n",
    "numerictest = cat2numeric(adulttest);\n",
    "\n",
    "%% Change the ratio between training dataset and testing dataset. 85% training 15% testing\n",
    "\n",
    "total_data = [numericdata; numerictest];\n",
    "num_training_data = round(size(total_data,1)*0.85); % 85 : training 15 : testing\n",
    "training_dataset = total_data(1:num_training_data,:);\n",
    "testing_dataset = total_data(num_training_data+1:end,:);\n",
    "\n",
    "%% Do Linear Regression which goals to predict the value between 0~1.\n",
    "\n",
    "% 1. Set inital learning rate, and weights (91d to also consider weight for bias)\n",
    "learning_rate = 0.03;\n",
    "weight = normrnd(1, 0.02, [1,91]); % 91d row vector\n",
    "iter = 0;\n",
    "\n",
    "% 2. Linear Regression Model Architecture\n",
    "epoch = 7;\n",
    "\n",
    "for e = [1:epoch] % iterate below.\n",
    "    \n",
    "    % 1.  shuffle rows of whole training dataset (sized 41516 data) :\n",
    "    shuffled_trds = training_dataset(randperm(size(training_dataset,1)),:);\n",
    "    \n",
    "    % 2.  Make a cell of mini-batches sized 100rows each, total 415\n",
    "    % minibatches.- every data participates in some mini batch.\n",
    "    mini_batches_cell = {};\n",
    "    for i = [0:414]\n",
    "        mini_batches_cell{end+1} = shuffled_trds(100*i+1:100*i+100,:);\n",
    "    end\n",
    "    % mini_batches_cell has 415 minibatches each consist of 100rows(91dimension).\n",
    "    \n",
    "    % 3.  Iterate for each minibatches and do linear regression gradient\n",
    "    % descent.\n",
    "    \n",
    "    for k = [1:length(mini_batches_cell)]\n",
    "        \n",
    "        now_mini_batch = mini_batches_cell{k}; % 100row, 91d including label\n",
    "        \n",
    "       % attaches ones to far left side\n",
    "        now_mini_batch = [ones(100,1), now_mini_batch]; %100row, 92d\n",
    "        \n",
    "        % seperate labels at the far right column as labels\n",
    "        infos = now_mini_batch(:,1:end-1); %100row, 91d combination of columns except labels\n",
    "        labels = now_mini_batch(:,end); % 100row, 1d the labels\n",
    "        \n",
    "        % get linear regression residual per each instances(row)\n",
    "        residuals = sum(weight.*infos,2)-labels;\n",
    "        \n",
    "        % updata weight vector\n",
    "        for j = [1:length(weight)]\n",
    "            gradient = (1./size(infos,1) .* sum(infos(:,j).*residuals));\n",
    "            weight(j) = weight(j) - learning_rate .* gradient ; % <= (gradient)\n",
    "            iter = iter + 1;\n",
    "        end\n",
    "        \n",
    "        % check if weight reached tolerence right after update is done\n",
    "        if norm(weight,2) <= 1e-6\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    % display at the end of each epoch how many weight updates so far and\n",
    "    % l_2_norm of gradient\n",
    "    iter\n",
    "    l_2_norm_gradient = norm(gradient,2)\n",
    "    \n",
    "end\n",
    "\n",
    "%% Check the performance of the trained weight with testing dataset.\n",
    "testing_label = testing_dataset(:,end); %labels\n",
    "testing_dataset = [ones(size(testing_dataset,1),1),testing_dataset(:,1:end-1)]; %all the columns left to label\n",
    "\n",
    "linear_regression_scores = sum(weight.*testing_dataset,2);\n",
    "linear_regression_scores(linear_regression_scores > 0.5) = 1;\n",
    "linear_regression_scores(linear_regression_scores <= 0.5) = 0;\n",
    "\n",
    "%% Check Matlab \\ solver performance\n",
    "training_l = training_dataset(:,end);\n",
    "training_info = [ones(size(training_dataset,1),1), training_dataset(:,1:end-1)];\n",
    "\n",
    "matlab_weights = training_info \\ training_l;\n",
    "m_regression_scores = sum(matlab_weights'.*testing_dataset,2);\n",
    "m_regression_scores(m_regression_scores > 0.5) = 1;\n",
    "m_regression_scores(m_regression_scores <= 0.5) = 0;\n",
    "\n",
    "%% print out our linear regrssion model accuracy and the matlab \\ accuracy\n",
    "Linear_regression_accuracy = round(mean(testing_label == linear_regression_scores)*100)\n",
    "m_regression_accuracy = round(mean(testing_label == m_regression_scores)*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><font color='red'>3. Rogistic Regression </font></h2>   \n",
    "\n",
    "* Our Logistic Regression model accepts 91-d array as an input and try to find a set of weights that makes up boundary that minimizes the following error function. We used ‘Gradient Descent’ to optimize the set of weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "total_weight_updates =\n",
      "\n",
      "       37765\n",
      "\n",
      "\n",
      "l_2_norm_gradient =\n",
      "\n",
      "    0.0050\n",
      "\n",
      "\n",
      "total_weight_updates =\n",
      "\n",
      "       75530\n",
      "\n",
      "\n",
      "l_2_norm_gradient =\n",
      "\n",
      "    0.0030\n",
      "\n",
      "\n",
      "total_weight_updates =\n",
      "\n",
      "      113295\n",
      "\n",
      "\n",
      "l_2_norm_gradient =\n",
      "\n",
      "   1.9414e-04\n",
      "\n",
      "\n",
      "total_weight_updates =\n",
      "\n",
      "      151060\n",
      "\n",
      "\n",
      "l_2_norm_gradient =\n",
      "\n",
      "    0.0080\n",
      "\n",
      "\n",
      "total_weight_updates =\n",
      "\n",
      "      188825\n",
      "\n",
      "\n",
      "l_2_norm_gradient =\n",
      "\n",
      "    0.0080\n",
      "\n",
      "\n",
      "total_weight_updates =\n",
      "\n",
      "      226590\n",
      "\n",
      "\n",
      "l_2_norm_gradient =\n",
      "\n",
      "   2.9454e-04\n",
      "\n",
      "\n",
      "total_weight_updates =\n",
      "\n",
      "      264355\n",
      "\n",
      "\n",
      "l_2_norm_gradient =\n",
      "\n",
      "    0.0050\n",
      "\n",
      "\n",
      "total_weight_updates =\n",
      "\n",
      "      302120\n",
      "\n",
      "\n",
      "l_2_norm_gradient =\n",
      "\n",
      "     0\n",
      "\n",
      "\n",
      "total_weight_updates =\n",
      "\n",
      "      339885\n",
      "\n",
      "\n",
      "l_2_norm_gradient =\n",
      "\n",
      "    0.0145\n",
      "\n",
      "\n",
      "total_weight_updates =\n",
      "\n",
      "      377650\n",
      "\n",
      "\n",
      "l_2_norm_gradient =\n",
      "\n",
      "    0.0088\n",
      "\n",
      "\n",
      "accuracy =\n",
      "\n",
      "    83\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%% Load data : testing dataset\n",
    "load census1994.mat\n",
    "\n",
    "numericdata = cat2numeric(adultdata);\n",
    "numerictest = cat2numeric(adulttest);\n",
    "\n",
    "%% Change the ratio between training dataset and testing dataset. 85% training 15% testing\n",
    "\n",
    "total_data = [numericdata; numerictest];\n",
    "num_training_data = round(size(total_data,1)*0.85); % 85 : training 15 : testing\n",
    "training_dataset = total_data(1:num_training_data,:);\n",
    "testing_dataset = total_data(num_training_data+1:end,:);\n",
    "\n",
    "%% Do Logistic Classification\n",
    "\n",
    "% 1. Set inital learning rate, and weights (91d to also consider weight for bias)\n",
    "learning_rate = 0.03;\n",
    "weight = normrnd(1, 0.2, [1,91]); % 91d row vector\n",
    "\n",
    "% 2. Logistic Classification Model Architecture\n",
    "\n",
    "% Set epoch\n",
    "epoch = 10; %select epoch\n",
    "iter = 0; %the number of weight update\n",
    "\n",
    "for e = [1:epoch] % Iterate belows for each epochs\n",
    "    \n",
    "    % 1.  shuffle rows of whole training dataset (sized 41516 data) :\n",
    "    shuffled_trds = training_dataset(randperm(size(training_dataset,1)),:);\n",
    "    \n",
    "    % 2.  Make a cell of mini-batches sized 100rows each, in total 415 of\n",
    "    % those. every data participates for each epoch.\n",
    "    mini_batches_cell = {};\n",
    "    for i = [0:414]\n",
    "        mini_batches_cell{end+1} = shuffled_trds(100*i+1:100*i+100,:);\n",
    "    end\n",
    "    % mini_batches_cell has 415minibatches each consist of 100rows.\n",
    "    \n",
    "    \n",
    "    % 3.  Iterate for each minibatches and do gradient descent.\n",
    "    for k = [1:length(mini_batches_cell)]\n",
    "        \n",
    "        now_mini_batch = mini_batches_cell{k}; % 100row, 91d including label\n",
    "        \n",
    "       % attaches ones to far left side for bias\n",
    "        now_mini_batch = [ones(100,1), now_mini_batch]; %100row, 92d\n",
    "        \n",
    "        % seperate labels at the far right column as separate column vector\n",
    "        % : labels, all column to the left is informations to be matched to\n",
    "        % the label.\n",
    "        \n",
    "        infos = now_mini_batch(:,1:end-1); %100row, 91d combination of columns except labels\n",
    "        labels = now_mini_batch(:,end); % 100row, 1d the labels\n",
    "        \n",
    "        % compute and get logistic regression residual for each row\n",
    "        % it's to be used for computing gradient later.\n",
    "        residuals = 1./(1+e.^(-1.*sum(weight.*infos,2)))-labels; \n",
    "        \n",
    "        % updata weight vector by gradient descent.\n",
    "        for j = [1:length(weight)]\n",
    "            gradient = (1./size(infos,1) .* sum(infos(:,j).*residuals));\n",
    "            weight(j) = weight(j) - learning_rate.*gradient; % <= ((sum(infos(:,j).*residuals))) : gradient\n",
    "            iter = iter + 1; % +1 to calculate total number of updates\n",
    "        end\n",
    "    \n",
    "    end\n",
    "   \n",
    "    % display for each epoch the changes in l_2 norm of gradient \n",
    "    % and number of total weight update\n",
    "    \n",
    "    total_weight_updates = iter\n",
    "    l_2_norm_gradient = norm(gradient,2)\n",
    "end\n",
    "\n",
    "%% Check the performance of the trained weight with testing dataset.\n",
    "testing_label = testing_dataset(:,end);\n",
    "testing_dataset = [ones(size(testing_dataset,1),1),testing_dataset(:,1:end-1)];\n",
    "logistic_regression_score = 1./(1+e.^(-1.*sum(weight.*testing_dataset,2)));\n",
    "logistic_regression_score(logistic_regression_score > 0.5) = 1;\n",
    "logistic_regression_score(logistic_regression_score <= 0.5) = 0;\n",
    "\n",
    "%% print out our linear regrssion model accuracy (stopped to grow after approx. 10epochs)\n",
    "accuracy = round(mean(testing_label == logistic_regression_score)*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><font color='red'>4. Deep Neural Network </font></h2>\n",
    "\n",
    "* The network includes 1). Input Layer – 2). Fully Connected Layers – 3). Softmax Layer. The built-in ‘trainNetwork’ function was used to train the network. After the network was trained, the validation data was classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on single CPU.\n",
      "|======================================================================================================================|\n",
      "|  Epoch  |  Iteration  |  Time Elapsed  |  Mini-batch  |  Validation  |  Mini-batch  |  Validation  |  Base Learning  |\n",
      "|         |             |   (hh:mm:ss)   |   Accuracy   |   Accuracy   |     Loss     |     Loss     |      Rate       |\n",
      "|======================================================================================================================|\n",
      "|       1 |           1 |       00:00:05 |       39.06% |       75.55% |       0.6939 |       0.6742 |          0.0300 |\n",
      "|       1 |          50 |       00:00:11 |       74.22% |              |       0.5665 |              |          0.0300 |\n",
      "|       1 |         100 |       00:00:16 |       76.56% |       75.55% |       0.5431 |       0.5563 |          0.0300 |\n",
      "|       1 |         150 |       00:00:21 |       73.44% |              |       0.5891 |              |          0.0300 |\n",
      "|       1 |         200 |       00:00:27 |       78.12% |       75.55% |       0.5269 |       0.5573 |          0.0300 |\n",
      "|       1 |         250 |       00:00:34 |       75.78% |              |       0.5568 |              |          0.0300 |\n"
     ]
    }
   ],
   "source": [
    "%% 종호 processing.m\n",
    "load census1994.mat\n",
    "\n",
    "numericdata = cat2numeric(adultdata);\n",
    "numerictest = cat2numeric(adulttest);\n",
    "\n",
    "%%\n",
    "\n",
    "mydata = [numericdata; numerictest];\n",
    "\n",
    "% Split data into training set and validation set\n",
    "samplenum = size(mydata, 1);\n",
    "trainnum = round(0.85*samplenum);\n",
    "testnum = round(0.15*samplenum);\n",
    "trainIdx = randi(samplenum, [1, trainnum]);\n",
    "testIdx = randi(samplenum, [1, testnum]);\n",
    "% training data\n",
    "tempXtrain = mydata(trainIdx, [1:90]);\n",
    "Ytrain = categorical(mydata(trainIdx, 91));\n",
    "for i=1:trainnum\n",
    "    Xtrain(i,1) = {tempXtrain(i,:)};\n",
    "end\n",
    "% validation data (test data)\n",
    "tempXtest = mydata(testIdx, [1:90]);\n",
    "Ytest = categorical(mydata(testIdx, 91));\n",
    "for i=1:testnum\n",
    "    Xtest(i,1) = {tempXtest(i,:)};\n",
    "end\n",
    "testds = {Xtest; Ytest};    % test datastore\n",
    "\n",
    "% Define network architecture\n",
    "layers = [\n",
    "    sequenceInputLayer(1)\n",
    "    lstmLayer(10,'OutputMode','last')\n",
    "    dropoutLayer(0.1)\n",
    "    fullyConnectedLayer(16)\n",
    "    fullyConnectedLayer(16)\n",
    "    fullyConnectedLayer(32)\n",
    "    dropoutLayer(0.1)\n",
    "    fullyConnectedLayer(2)\n",
    "    softmaxLayer\n",
    "    classificationLayer];\n",
    "\n",
    "% training options\n",
    "options = trainingOptions('sgdm', ...\n",
    "    'InitialLearnRate',0.03, ...\n",
    "    'MaxEpochs',3, ...\n",
    "    'ValidationData',testds, ...\n",
    "    'ValidationFrequency',100, ...\n",
    "    'Shuffle','every-epoch', ...\n",
    "    'Verbose',true, ...\n",
    "    'Plots','training-progress');\n",
    "\n",
    "% Train\n",
    "net = trainNetwork(Xtrain,Ytrain,layers,options);\n",
    "\n",
    "% Classify Validation data and Compute Accuracy\n",
    "predictedLabels = classify(net,Xtest);\n",
    "valLabels = Ytest;\n",
    "accuracy = (sum(predictedLabels == valLabels)/numel(valLabels));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Result analysis is posted in the url => [Result_Analysis](https://drive.google.com/file/d/1xWQrm-9VBgGtEUKNsDHqPv4m1axLx0o1/view?usp=sharing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Matlab",
   "language": "matlab",
   "name": "matlab"
  },
  "language_info": {
   "codemirror_mode": "octave",
   "file_extension": ".m",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-octave",
   "name": "matlab",
   "version": "0.16.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
